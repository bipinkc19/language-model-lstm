{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a0jHYdIiU4is",
    "outputId": "d9187d0f-52e3-4378-8157-3fa035ce5b16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "BnpS54ZtWI-9",
    "outputId": "f2db6f7b-800b-42af-f875-34730e29ed43"
   },
   "outputs": [],
   "source": [
    "# # Run this cell to mount your Google Drive.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s15AGb3wW2Ym",
    "outputId": "4462062b-0b8b-4fc4-9f14-7e5ead2446b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in characters: 5445610\n"
     ]
    }
   ],
   "source": [
    "with open('./data/shakespeare.txt', 'r') as f:\n",
    "      text = f.read().lower()\n",
    "print('Corpus length in characters:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decreasing the corpus size for memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsByCpjVi3Mf"
   },
   "outputs": [],
   "source": [
    "text = text[:3000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9V28m2vwOBSX",
    "outputId": "168c9d15-388a-4606-bb14-15efb8075abc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Unique Characters: 57\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('Total Number of Unique Characters:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars)) # Character to index\n",
    "indices_char = dict((i, c) for c, i in char_indices.items()) # Index to Character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0X5s_EGRQBLz"
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(len(text) - max_len):\n",
    "    sentence = text[i: i + max_len]\n",
    "    next_char = text[i + max_len]\n",
    "    sentences.append(sentence)\n",
    "    next_chars.append(next_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Releasing memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WuMwgbfTjGj3"
   },
   "outputs": [],
   "source": [
    "text = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders for training in a supervised environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evuBBaS8Vs6s"
   },
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), max_len, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LNyOYuiXfib"
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        # Populate Tensor Input\n",
    "        x[i, t, char_indices[char]] = 1 \n",
    "    # Populate y with the character just after the sequence\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0T821LnmgHC"
   },
   "outputs": [],
   "source": [
    "filepath = '/content/drive/My Drive/dataset/saved_weights_language_model.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, save_best_only=False, save_weights_only=False)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked LSTM with 2 layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "7WXu9xJYZFyw",
    "outputId": "6d64bfb1-ea86-4fe8-92a8-65d9c9e7fd50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0710 04:00:34.734450 139881602815872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0710 04:00:34.772166 139881602815872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0710 04:00:34.778587 139881602815872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0710 04:00:35.695537 139881602815872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0710 04:00:35.706248 139881602815872 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0710 04:00:35.747350 139881602815872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0710 04:00:35.756458 139881602815872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0710 04:00:35.894264 139881602815872 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2969950 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      "2969950/2969950 [==============================] - 3320s 1ms/step - loss: 1.6156 - val_loss: 1.3621\n",
      "Epoch 2/5\n",
      "2969950/2969950 [==============================] - 3325s 1ms/step - loss: 1.3724 - val_loss: 1.3098\n",
      "Epoch 3/5\n",
      "1800960/2969950 [=================>............] - ETA: 21:44 - loss: 1.3374Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "# Size of vector in the hidden layer.\n",
    "hidden_size = 256\n",
    "# Initialize Sequential Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_size, input_shape=(max_len, len(chars)), return_sequences=True))\n",
    "model.add(LSTM(hidden_size))\n",
    "model.add(Dropout(0.5))\n",
    "# Add the output layer that is a softmax of the number of characters\n",
    "model.add(Dense(len(chars), activation='softmax')) \n",
    "# Optimization through RMSprop\n",
    "optimizer_new = RMSprop() \n",
    "# Consider cross Entropy loss. Why? MLE of P(D | theta)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer_new) \n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=5, \n",
    "          validation_split=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bgJ8t7prc5us"
   },
   "outputs": [],
   "source": [
    "model.save_weights('/content/drive/My Drive/dataset/saved_weights_language_model.hdf5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOWTn_zTdGRw"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.):\n",
    "    \"\"\"Perform Temperature Sampling\"\"\"\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    # Softmax of predictions\n",
    "    preds = exp_preds / np.sum(exp_preds) \n",
    "    # Sample a single characters, with probabilities defined in `preds`\n",
    "    probas = np.random.multinomial(1, preds[0], 1) \n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with differnt seeds and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "-ytt1SgWe37c",
    "outputId": "dcc9ea70-63d8-4b5c-c68e-a99bdf4903f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed ==== ie,\n",
      "  but as the riper should by time decease,\n",
      "  h\n",
      "\n",
      " Machine written paragraph \n",
      " ie,\n",
      "  but as the riper should by time decease,\n",
      "  how known the duke of same in the captain,\n",
      "    signify that the duke of clifford and i say i have no tongue\n",
      "    that he a one he said your captain of herself within\n",
      "    with the slack of thee, and in death endure to me do\n",
      "    a country to follow the sake of the world,\n",
      "    and privilent cardinal hath look his answers.\n",
      "  king henry. good my lord, make me the king.\n",
      "                                    \n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[111]\n",
    "print('seed ====', sentence)\n",
    "\n",
    "temperature = 0.6\n",
    "for i in range(400):\n",
    "    text = sentence[i: i + max_len]\n",
    "    x_to_be_pred = np.zeros((1, max_len, len(chars)))\n",
    "    for t, char in enumerate(text):\n",
    "        x_to_be_pred[0, t, char_indices[char]] = 1.\n",
    "    ans = model.predict(x_to_be_pred)\n",
    "    predicted_char = indices_char[sample(ans, temperature)]\n",
    "    sentence += predicted_char\n",
    "\n",
    "print('\\n Machine written paragraph \\n', sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "E31RcsuBfIRH",
    "outputId": "b843b637-8399-4ea7-8aa2-2f212131dfb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed ==== for she is the beauty of the skin that holds in my\n",
      "\n",
      " Machine written paragraph \n",
      " for she is the beauty of the skin that holds in my ladies.\n",
      "    a day\n",
      "  and love treader resald'd him  \n",
      "   itned rich peraltian moes.\n",
      "  second lord. and all king shall ill lead in once carried to these,\n",
      "    this time to th' king to   making fear\n",
      "    of walks; shall seem the knowly ear. cave malice, further\n",
      "    glory pointcred and a countrypefalle;\n",
      "    and as the man of a dead bearford one\n",
      "    you were the comes to prive biy, and give the world.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sentence = 'for she is the beauty of the skin that holds in my'\n",
    "print('seed ====', sentence)\n",
    "\n",
    "temperature = 1.0\n",
    "for i in range(400):\n",
    "    text = sentence[i: i + max_len]\n",
    "    x_to_be_pred = np.zeros((1, max_len, len(chars)))\n",
    "    for t, char in enumerate(text):\n",
    "        x_to_be_pred[0, t, char_indices[char]] = 1.\n",
    "    ans = model.predict(x_to_be_pred)\n",
    "    predicted_char = indices_char[sample(ans, temperature)]\n",
    "    sentence += predicted_char\n",
    "\n",
    "print('\\n Machine written paragraph \\n', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "hF59ZtHkHi8n",
    "outputId": "c22ad42c-c16a-40d3-f609-b725e2075f50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed ==== for she is the beauty of the skin that holds in my\n",
      "\n",
      " Machine written paragraph \n",
      " for she is the beauty of the skin that holds in my ho't cit whithfei!cbur\n",
      "  h; likeferved mry? driy, hycks,sol.\n",
      "  first citizen. it' cushiot yz'orail:\n",
      "  well hu bass, caa i bags;\n",
      "    oo, aler; a < wess; or tuquguys-\n",
      "  oph. weoi'y hemy swa. kmy oxly,\n",
      "     reus;. take myhpkilr.hingdon.\n",
      "  orlando.egacmbefrude's. doltelogt] vellus\n",
      " \n",
      "  iardinna. , bitionmera.\n",
      "\n",
      "  cactori]-bidst?\n",
      "  riched ho,[it with vac\n",
      "    byu itquepir exepegitf\n",
      "   oorsqu't grestgomje\n"
     ]
    }
   ],
   "source": [
    "sentence = 'for she is the beauty of the skin that holds in my'\n",
    "print('seed ====', sentence)\n",
    "\n",
    "temperature = 2.0\n",
    "for i in range(400):\n",
    "    text = sentence[i: i + max_len]\n",
    "    x_to_be_pred = np.zeros((1, max_len, len(chars)))\n",
    "    for t, char in enumerate(text):\n",
    "        x_to_be_pred[0, t, char_indices[char]] = 1.\n",
    "    ans = model.predict(x_to_be_pred)\n",
    "    predicted_char = indices_char[sample(ans, temperature)]\n",
    "    sentence += predicted_char\n",
    "\n",
    "print('\\n Machine written paragraph \\n', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "9tkuqFWMHpAZ",
    "outputId": "046bcb1e-5d45-4c4d-ffb8-9f8aa4bb9a3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed ==== for she is the beauty of the skin that holds in my\n",
      "\n",
      " Machine written paragraph \n",
      " for she is the beauty of the skin that holds in my food and sound him a place\n",
      "    to make it as a good book of breath,\n",
      "    and the place of impotitions of the son,\n",
      "    and the great enemies that were great thing than i had remembers\n",
      "    the towns to the crown age should ack the torn with him,\n",
      "    i give me entertain'd the court in the man is a complexions,\n",
      "    in his hand of brother the day.\n",
      "  holofernes. i will not come, i would i am sure as the\n"
     ]
    }
   ],
   "source": [
    "sentence = 'for she is the beauty of the skin that holds in my'\n",
    "print('seed ====', sentence)\n",
    "\n",
    "temperature = 0.6\n",
    "for i in range(400):\n",
    "    text = sentence[i: i + max_len]\n",
    "    x_to_be_pred = np.zeros((1, max_len, len(chars)))\n",
    "    for t, char in enumerate(text):\n",
    "        x_to_be_pred[0, t, char_indices[char]] = 1.\n",
    "    ans = model.predict(x_to_be_pred)\n",
    "    predicted_char = indices_char[sample(ans, temperature)]\n",
    "    sentence += predicted_char\n",
    "\n",
    "print('\\n Machine written paragraph \\n', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "4Uk3hg4pgQbH",
    "outputId": "61b4c961-7be2-4239-d90a-532e0bb552a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed ==== for she is the beauty of the skin that holds in my\n",
      "\n",
      " Machine written paragraph \n",
      " for she is the beauty of the skin that holds in my son,\n",
      "    and the state of the parts of the country,\n",
      "    and the country that i say the country that i say the prince of such a service\n",
      "    that we will be the country than the seal of the countrymen,\n",
      "    and the country that i will not be a soul\n",
      "    and the first and the country and the state\n",
      "    that we shall be the state of the state of the state\n",
      "    and the state of the country shall be so sou\n"
     ]
    }
   ],
   "source": [
    "sentence = 'for she is the beauty of the skin that holds in my'\n",
    "print('seed ====', sentence)\n",
    "\n",
    "temperature = 0.1\n",
    "for i in range(400):\n",
    "    text = sentence[i: i + max_len]\n",
    "    x_to_be_pred = np.zeros((1, max_len, len(chars)))\n",
    "    for t, char in enumerate(text):\n",
    "        x_to_be_pred[0, t, char_indices[char]] = 1.\n",
    "    ans = model.predict(x_to_be_pred)\n",
    "    predicted_char = indices_char[sample(ans, temperature)]\n",
    "    sentence += predicted_char\n",
    "\n",
    "print('\\n Machine written paragraph \\n', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BDuYE4whhBZv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "language_model",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
