{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a0jHYdIiU4is",
    "outputId": "d733ca45-e4be-46ba-bb27-0b55d7fbf3af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BnpS54ZtWI-9",
    "outputId": "11876ac2-c385-46cd-a915-deda8e72e267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# # Run this cell to mount your Google Drive.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s15AGb3wW2Ym",
    "outputId": "8e188742-8178-465f-bf0a-f72f2df3ea92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in characters: 46609\n"
     ]
    }
   ],
   "source": [
    "with open('/data/chanda.txt', 'r') as f:\n",
    "      text = f.read().lower()\n",
    "print('Corpus length in characters:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsByCpjVi3Mf"
   },
   "outputs": [],
   "source": [
    "text = text[:3000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9V28m2vwOBSX",
    "outputId": "8927e36e-6dc9-4471-9c72-02ac23eba591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Unique Characters: 83\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('Total Number of Unique Characters:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars)) # Character to index\n",
    "indices_char = dict((i, c) for c, i in char_indices.items()) # Index to Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0X5s_EGRQBLz"
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(len(text) - max_len):\n",
    "  sentence = text[i: i + max_len]\n",
    "  next_char = text[i + max_len]\n",
    "  sentences.append(sentence)\n",
    "  next_chars.append(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WuMwgbfTjGj3"
   },
   "outputs": [],
   "source": [
    "text = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evuBBaS8Vs6s"
   },
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), max_len, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LNyOYuiXfib"
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        # Populate Tensor Input\n",
    "        x[i, t, char_indices[char]] = 1 \n",
    "    # Populate y with the character just after the sequence\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0T821LnmgHC"
   },
   "outputs": [],
   "source": [
    "filepath = '/content/drive/My Drive/dataset/saved_weights_language_model.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, save_best_only=False, save_weights_only=False)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7WXu9xJYZFyw",
    "outputId": "5bfb9a90-bc52-48dd-eaba-6edaf5018e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Train on 46093 samples, validate on 466 samples\n",
      "Epoch 1/30\n",
      "46093/46093 [==============================] - 57s 1ms/step - loss: 3.3515 - val_loss: 3.1279\n",
      "Epoch 2/30\n",
      "46093/46093 [==============================] - 58s 1ms/step - loss: 2.8508 - val_loss: 2.8722\n",
      "Epoch 3/30\n",
      "46093/46093 [==============================] - 56s 1ms/step - loss: 2.6907 - val_loss: 2.7831\n",
      "Epoch 4/30\n",
      "46093/46093 [==============================] - 57s 1ms/step - loss: 2.5869 - val_loss: 2.7457\n",
      "Epoch 5/30\n",
      "46093/46093 [==============================] - 57s 1ms/step - loss: 2.5018 - val_loss: 2.6862\n",
      "Epoch 6/30\n",
      "46093/46093 [==============================] - 57s 1ms/step - loss: 2.4259 - val_loss: 2.6136\n",
      "Epoch 7/30\n",
      "46093/46093 [==============================] - 57s 1ms/step - loss: 2.3529 - val_loss: 2.5832\n",
      "Epoch 8/30\n",
      "46093/46093 [==============================] - 56s 1ms/step - loss: 2.2869 - val_loss: 2.5808\n",
      "Epoch 9/30\n",
      "46093/46093 [==============================] - 56s 1ms/step - loss: 2.2201 - val_loss: 2.5305\n",
      "Epoch 10/30\n",
      "46093/46093 [==============================] - 56s 1ms/step - loss: 2.1502 - val_loss: 2.5288\n",
      "Epoch 11/30\n",
      "46093/46093 [==============================] - 57s 1ms/step - loss: 2.0853 - val_loss: 2.5851\n",
      "Epoch 12/30\n",
      "46093/46093 [==============================] - 57s 1ms/step - loss: 2.0163 - val_loss: 2.5089\n",
      "Epoch 13/30\n",
      "46093/46093 [==============================] - 56s 1ms/step - loss: 1.9471 - val_loss: 2.5297\n",
      "Epoch 14/30\n",
      "46093/46093 [==============================] - 57s 1ms/step - loss: 1.8783 - val_loss: 2.5315\n",
      "Epoch 15/30\n",
      "46093/46093 [==============================] - 55s 1ms/step - loss: 1.8077 - val_loss: 2.5489\n",
      "Epoch 16/30\n",
      "46093/46093 [==============================] - 56s 1ms/step - loss: 1.7317 - val_loss: 2.5952\n",
      "Epoch 17/30\n",
      "46093/46093 [==============================] - 55s 1ms/step - loss: 1.6637 - val_loss: 2.6243\n",
      "Epoch 18/30\n",
      "46093/46093 [==============================] - 57s 1ms/step - loss: 1.5917 - val_loss: 2.6859\n",
      "Epoch 19/30\n",
      "46093/46093 [==============================] - 57s 1ms/step - loss: 1.5195 - val_loss: 2.6666\n",
      "Epoch 20/30\n",
      "46093/46093 [==============================] - 55s 1ms/step - loss: 1.4434 - val_loss: 2.6354\n",
      "Epoch 21/30\n",
      "46093/46093 [==============================] - 56s 1ms/step - loss: 1.3770 - val_loss: 2.7393\n",
      "Epoch 22/30\n",
      "46093/46093 [==============================] - 56s 1ms/step - loss: 1.3111 - val_loss: 2.8432\n",
      "Epoch 23/30\n",
      "46093/46093 [==============================] - 55s 1ms/step - loss: 1.2440 - val_loss: 2.8746\n",
      "Epoch 24/30\n",
      "46093/46093 [==============================] - 56s 1ms/step - loss: 1.1851 - val_loss: 2.8116\n",
      "Epoch 25/30\n",
      "46093/46093 [==============================] - 55s 1ms/step - loss: 1.1237 - val_loss: 3.0102\n",
      "Epoch 26/30\n",
      "46093/46093 [==============================] - 55s 1ms/step - loss: 1.0646 - val_loss: 2.9841\n",
      "Epoch 27/30\n",
      "46093/46093 [==============================] - 57s 1ms/step - loss: 1.0126 - val_loss: 3.0944\n",
      "Epoch 28/30\n",
      "46093/46093 [==============================] - 56s 1ms/step - loss: 0.9574 - val_loss: 3.1365\n",
      "Epoch 29/30\n",
      "46093/46093 [==============================] - 55s 1ms/step - loss: 0.9193 - val_loss: 3.1926\n",
      "Epoch 30/30\n",
      "46093/46093 [==============================] - 57s 1ms/step - loss: 0.8715 - val_loss: 3.2281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae3a330198>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Building model...')\n",
    "# Size of vector in the hidden layer.\n",
    "hidden_size = 256\n",
    "# Initialize Sequential Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_size, input_shape=(max_len, len(chars)), return_sequences=True))\n",
    "model.add(LSTM(hidden_size))\n",
    "model.add(Dropout(0.5))\n",
    "# Add the output layer that is a softmax of the number of characters\n",
    "model.add(Dense(len(chars), activation='softmax')) \n",
    "# Optimization through RMSprop\n",
    "optimizer_new = RMSprop() \n",
    "# Consider cross Entropy loss. Why? MLE of P(D | theta)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer_new) \n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=30, \n",
    "          validation_split=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bgJ8t7prc5us"
   },
   "outputs": [],
   "source": [
    "model.save_weights('/content/drive/My Drive/dataset/saved_weights_language_model.hdf5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOWTn_zTdGRw"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.):\n",
    "    \"\"\"Perform Temperature Sampling\"\"\"\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    # Softmax of predictions\n",
    "    preds = exp_preds / np.sum(exp_preds) \n",
    "    # Sample a single characters, with probabilities defined in `preds`\n",
    "    probas = np.random.multinomial(1, preds[0], 1) \n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "-ytt1SgWe37c",
    "outputId": "1db84d99-161a-4751-9314-01648f7d6e19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed ==== क्क बेखबर भाे खाेजे सबै ती कुना\n",
      "लुट्छाै मात्र कि द\n",
      "\n",
      " Machine written paragraph \n",
      " क्क बेखबर भाे खाेजे सबै ती कुना\n",
      "लुट्छाै मात्र कि देख कनेकाे मुन्छस्छ।\n",
      "शाे भुन्छ जन्याे छ छन ।।\n",
      "खुका होमा सदे बेखा मसमा।।\n",
      "\n",
      "गाे काे काे जो पार।।\n",
      "मातेर हो मान सुव जर्दै अन्।।\n",
      "\n",
      "हाँ दान बोखा बने बधा सवै होश्छ।\n",
      "पन्द्छ भाल अथिका छ लौम्यो न भनु ।\n",
      "\n",
      "पारो केर मारा नागनै भनि तिर।।\n",
      "\n",
      "हेर हुम्र जागाँ मसना दुश खिन्द नाव्याा।\n",
      "हिन्दै बस्न भुन्छ न्याउँनै माग्र।।\n",
      "\n",
      "सु सुन्दछ को दुख जन्दछ काे।\n",
      "पार्दा तिरा ता काे सतै यहे ।।\n",
      "\n",
      "माेको हुन् पार्छ साहा रोध हाे कधान।।\n",
      "काि किर\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[111]\n",
    "print('seed ====', sentence)\n",
    "\n",
    "temperature = 0.6\n",
    "for i in range(400):\n",
    "    text = sentence[i: i + max_len]\n",
    "    x_to_be_pred = np.zeros((1, max_len, len(chars)))\n",
    "    for t, char in enumerate(text):\n",
    "        x_to_be_pred[0, t, char_indices[char]] = 1.\n",
    "    ans = model.predict(x_to_be_pred)\n",
    "    predicted_char = indices_char[sample(ans, temperature)]\n",
    "    sentence += predicted_char\n",
    "\n",
    "print('\\n Machine written paragraph \\n', sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "v-AJnAJahxIO",
    "outputId": "2b4c5998-83e2-48c8-b9ad-4bdf44fb0e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed ==== र मेराे मनमा बसेकाे।।२।।\n",
      "\n",
      "लाखौं लुटेरा अब लुट्न आए\n",
      "\n",
      " Machine written paragraph \n",
      " र मेराे मनमा बसेकाे।।२।।\n",
      "\n",
      "लाखौं लुटेरा अब लुट्न आए।\n",
      "बासाब बाहिना निएको छ सुनादया ।।\n",
      "\n",
      "स्वास्था पनि निर्व जानफना भन्दैन कित्मा नयो\n",
      "हाम्रा सूर जनमा सबै अनि सम्मिन्ध्यो मनाती ति।\n",
      "\n",
      "मारा राख्छ देख भनिका सब हुन्छ जलेमा।\n",
      "नमान्ने कुनै भन ब बस्छन् रिन्छान्।\n",
      "नभाँका सबै बन्दछ पनि सुन्दर स्वास्थ ।।\n",
      "\n",
      "सान्दा र देशका तिममा पुल्याै।\n",
      "कतै भन्छ कानून् सब जानाै गर्नै नसक्ने भएँ,\n",
      "कोही काश र जानेको , सर्ना भनिन सर् कनि ।\n",
      "बाेकी ने भएकाे का, उन्छ सक्ता त पाग्यो।\n",
      "बारामा क\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[408]\n",
    "print('seed ====', sentence)\n",
    "\n",
    "temperature = 0.1\n",
    "for i in range(400):\n",
    "    text = sentence[i: i + max_len]\n",
    "    x_to_be_pred = np.zeros((1, max_len, len(chars)))\n",
    "    for t, char in enumerate(text):\n",
    "        x_to_be_pred[0, t, char_indices[char]] = 1.\n",
    "    ans = model.predict(x_to_be_pred)\n",
    "    predicted_char = indices_char[sample(ans, temperature)]\n",
    "    sentence += predicted_char\n",
    "\n",
    "print('\\n Machine written paragraph \\n', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "aq46W6E5fxGW",
    "outputId": "a5697207-71b3-447c-c7e5-5c17ccb51a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed ==== र मेराे मनमा बसेकाे।।२।।\n",
      "\n",
      "लाखौं लुटेरा अब लुट्न आए\n",
      "\n",
      " Machine written paragraph \n",
      " र मेराे मनमा बसेकाे।।२।।\n",
      "\n",
      "लाखौं लुटेरा अब लुट्न आए।\n",
      "जुवथाल विघुी सती जिरण्गा भए पबाले नमो,\n",
      "कने आुन धां हुदेन एक तन्भा\n",
      "गरैकार छ भाेच्न ेसा आफ्नु।फेराे म पाउाे\n",
      "नसाे देधी सम्व यकाे अबूतान।ु्छ।\n",
      "नेपाल र याे सककै नखान्र लीखराय्मतमाे।\n",
      "नयाँ गल्े पुगनर नोण्लास परी फम्नै भकिु झ ्ए ।\n",
      "–\n",
      "भानस्यु प्रशास्पहनुलोह,बराशु ंत्यो कित्थे टौऊँ४।\n",
      "\n",
      "हसमैका घर ंउर्ौले किम म्याग छ मातिर,।\n",
      "कणवयस मालुमा ए, नवै णश्क मनले गनिू,\n",
      "मातवमाका ललका बिता, सुणसलहर थत्पु आ\n",
      "दकिनन कसहँस ,ह\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[408]\n",
    "print('seed ====', sentence)\n",
    "\n",
    "temperature = 2\n",
    "for i in range(400):\n",
    "    text = sentence[i: i + max_len]\n",
    "    x_to_be_pred = np.zeros((1, max_len, len(chars)))\n",
    "    for t, char in enumerate(text):\n",
    "        x_to_be_pred[0, t, char_indices[char]] = 1.\n",
    "    ans = model.predict(x_to_be_pred)\n",
    "    predicted_char = indices_char[sample(ans, temperature)]\n",
    "    sentence += predicted_char\n",
    "\n",
    "print('\\n Machine written paragraph \\n', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hF59ZtHkHi8n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BDuYE4whhBZv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "language_model",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
